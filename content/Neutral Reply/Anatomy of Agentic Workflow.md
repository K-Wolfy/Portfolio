---
created: 2025-12-23T14:19
updated: 2025-12-25T00:04
publish: true
---
## The Anatomy of an Agentic Workflow


| **The Prompt Module**                                                                                                                                                                                                                                        | **The Code Implementation**                                                                                                                                                                                                                                                                                                                                                                                                    | **The User Experience**                                                                                                                                                                                                                                                                                                             |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Module 0 – Global Rules & Operational Mandate**<br/>Defines identity ("Neutral Communication Processor"), JADE prohibition, BIFF adherence, no inference, no therapeutic language, and Australian context.                                                 | **`app/lib/prompts/analyzePrompt.ts` & `app/lib/prompts/finalizePrompt.ts`**<br/>These modules export `ANALYSIS_SYSTEM_PROMPT` and `FINALIZE_SYSTEM_PROMPT`, which embed Module 0 as the top‑level contract for all model calls. The API routes (`/api/analyze`, `/api/finalize`) always pass these as the `system` value into `streamText`, ensuring every LLM invocation shares the same global operating rules.                                                                             | **Consistent, predictable behavior across steps.**<br/>The user never sees therapy talk, over‑friendly language, or over‑explaining. Every output (Filtered Points and final reply) feels like the same calm, professional engine, regardless of which model or deployment is currently in use.                                     |
| **Module 2 – Safety Check (Immediate Termination)**<br/>If threats, self‑harm, child abuse, or imminent danger are detected, output a single fixed "SAFETY FLAG" message and stop.                                                                           | **`app/lib/safety.ts` & API routes**<br/>`checkSafetyPatterns()` scans the input (message, or message+answers) for high‑risk phrases. In `/api/analyze/route.ts` and `/api/finalize/route.ts`, if `safety.flagged` is true, the handler logs a warning and returns `SAFETY_FLAG_MESSAGE` as plain text, bypassing the LLM entirely. The same wording is also specified in Module 2 of the system prompt, aligning model‑level behavior with code‑level gates.                                  | **Clear hard stop when safety risk is detected.**<br/>Instead of a reply, the user sees a single neutral warning: "This message contains content that may require legal or law enforcement attention. Do not respond…". This makes the product feel cautious and trustworthy in high‑risk scenarios.                                |
| **Module 3 – Categorization Ruleset (A/B/C/D)**<br/>Every segment must be classified as Logistics/Actionable, Informational, Emotional Bait/Opinion, or Required Reply.                                                                                      | **`app/lib/prompts/analyzePrompt.ts` → `ANALYSIS_SYSTEM_PROMPT`**<br/>Module 3 is encoded as natural‑language logic in the system prompt. The `/api/analyze` route packages the user message via `buildAnalysisUserContent(message)` and calls `streamText` with `ANALYSIS_SYSTEM_PROMPT`. The LLM performs the segmentation and labeling according to these instructions; there is no "chat logic" in code – categorization lives in the prompt layer by design.                              | **User sees structure, not chaos.**<br/>What started as a long, angry paragraph is transformed into a small list of neutral items. The user is implicitly guided to distinguish logistics and information from emotional bait, without being forced to understand the underlying A/B/C/D taxonomy.                                  |
| **Module 4 – Sanitization Procedure**<br/>Strip accusations and emotional language, keep only neutral, factual content. Explicitly drop Emotional Bait (C).                                                                                                  | **LLM behavior invoked by `/api/analyze` under Module 4 rules**<br/>The sanitization algorithm is expressed in natural language in `ANALYSIS_SYSTEM_PROMPT`. Code enforces the context and shape: it passes only the original message and expects only the "Filtered Points" section. Combined with `temperature: 0.2` in `aiConfig.ts`, this turns sanitization into a stable transformation step, not a creative one.                                                                        | **Clean, non‑triggering "Filtered Points."**<br/>The user sees only neutral, factual restatements of what they need to respond to. Insults and accusations vanish, reducing emotional load and making it easier to think clearly about the reply.                                                                                   |
| **Module 5 – Filtered Points Output Contract**<br/>Must output `## Filtered Points` and numbered `Information`/`Request` items with specific two‑line formats per point.                                                                                     | **`app/lib/prompts/analyzePrompt.ts` + `app/page.tsx`**<br/>The prompt dictates a precise Markdown schema; `/api/analyze` streams that text back. In the UI, `analysisOutput` is rendered by `<ReactMarkdown remarkPlugins={[remarkGfm]}>`, and `step1Locked` / `step2Active` in `app/page.tsx` depend on `!!analysisOutput`. The code effectively treats the prompt as a schema: when the heading and numbered items appear, the UI moves the user into Step 2 and shows the answer textarea. | **A guided checklist instead of free‑form AI text.**<br/>The user sees a clearly‑formatted "## Filtered Points" section with numbered "Information" and "Request" items. It looks like a form they can complete, not a blob of AI prose—making the workflow obvious and repeatable.                                                 |
| **Module 6 – Execution Rule (Analysis Only)**<br/>This endpoint must _never_ generate the reply; it only runs Safety → Categorization → Sanitization → Filtered Points.                                                                              | **Endpoint contract in `/api/analyze/route.ts`**<br/>The route only calls `streamText` with `ANALYSIS_SYSTEM_PROMPT` and returns `result.toTextStreamResponse()`. There is no code path that could generate a final email. Module 6 in the prompt reiterates this, ensuring the model itself is instructed to stop after Filtered Points.                                                                                                                                                      | **User understands "Step 1: Extract, Step 2: Answer, Step 3: Draft."**<br/>There is no confusion about the engine "jumping ahead." After analysis, they get only the distilled points and an input box for answers; the reply is clearly reserved for Step 3.                                                                       |
| **Finalization Prompt – Answer Interpretation & BIFF Reply**<br/>(In FINALIZE_SYSTEM_PROMPT) Define how to map messy answers (e.g., "1 sure", "2 nope", "3 Sunday 6/10 at 6pm") into clear yes/no/clarify commitments, and when to ask for more information. | **`app/lib/prompts/finalizePrompt.ts` + `/api/finalize/route.ts`**<br/>`FINALIZE_SYSTEM_PROMPT` contains detailed examples (A–I) explaining how to interpret short, emotional, or informal answers. `/api/finalize` passes `{ originalMessage, userAnswers }` through `buildFinalizeUserContent()` and sets `system: getFinalizeSystemPrompt(profile)`. The route is responsible for validation and safety; the prompt is responsible for mapping text answers to operational decisions.       | **User can answer in natural, messy language—but gets a clean email.**<br/>They might type "1. sure" or "2 nope" or "3 Sunday 6/10 at 6pm"; the engine still produces a crisp BIFF email that clearly states what they will and will not do, without reproducing the user's emotional phrasing.                                     |
| **Style Directives – Additional Style Rules (Formality/Tone/Structure/Length/Lexical)**<br/>Add narrowly scoped guidance ("cool tone", "structured lines", "plain words") on top of the base BIFF rules.                                                     | **`FinalizeProfile` in `promptTypes.ts` + `buildStyleDirectives(profile)` in `finalizePrompt.ts` + UI chips in `app/page.tsx`**<br/>The UI exposes style keys via chips (e.g., `biff_plain`, `tone_warm`, `structured`); `/api/finalize` maps those keys into a strongly‑typed `FinalizeProfile`. `getFinalizeSystemPrompt(profile)` appends a small natural‑language style block to `FINALIZE_SYSTEM_PROMPT`. The core BIFF/JADE rules remain intact; only surface style changes.             | **User feels in control of the "voice" without breaking safety.**<br/>They can choose Plain vs Formal, Cool vs Warm, Paragraph vs Lines, etc. The final email changes in tone and layout while still being neutral, brief, and legally appropriate—demonstrating a direct line from style preferences to prompt architecture to UI. |

This table makes the lineage explicit:

- **Natural Language Instruction** (Prompt Modules) →
- **TypeScript Execution** (API routes + shared libraries) →
- **UI Result** (what the user actually sees and can do).

Together, they show that you're not just writing prompts, but designing an **agentic workflow** that translates human requirements into a coherent technical architecture.
